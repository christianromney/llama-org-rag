#+TITLE: org-mode RAG Project with LlamaIndex
* Overview
This is my second attempt to index my org-mode files to support semantic search
and RAG.

** Vector Database
My previous experiments with Chroma were unsuccessful (embeddings weren't
associated with the org-file content). This version leverages LlamaIndex's
ability to persist an index as files on disk directly without need for a vector
store. Future versions may leverage a proper vector database such as Lance DB,
Qdrant, or Weaviate (embedded).

** LlamaIndex
*** Setup
In a fresh conda environment, install the basic dependencies for this project
with:
#+begin_src shell
pip3 install -r requirements.txt
#+end_src

*** Concepts
**** RAG Phases
***** Ingestion
- loading information from data sources (Documents comprised of Nodes)
- Connectors (aka Readers) know how to ingest particular formats and sources of data
****** Document
- an abstraction that acts like generic container for data
- also tracks metadata and relationships among data (e.g. parent/child or related Docs/Nodes)
****** Node
- a chunk of a source Document
- inherit Document metadata

***** Indexing
- Indexes are data structures that support efficient retrieval and query,
  particularly /semantic/ query
- uses vector Embeddings, which map words to numerical vectors such that related
  concepts are nearby in the vector space
***** Storage
- persist embeddings, content, and metadata durably, often in a vector database
***** Query
- Retrievers fetch context from an index to feed context of an LLM query
- Routers select the optimal retriever from one or more possible choices
- Node Post-processors transform retrieved nodes (e.g. filtering, re-ranking)
- Response synthesizer generates a response from an LLM using a query and
  retrieved data (orchestration?)
**** Composition
***** Query Engine
- end-to-end pipeline for producing a response to a query using an LLM and
  retrieved content
***** Chat Engine
- end-to-end pipeline for having chat (multiple back and forth Q&A)
***** Agents
- automated *decision maker* powered by an LLM that interacts with the world using
  Tools

*** Proof of Concept
The following code uses LlamaIndex to perform Retrieval Augmented Generation (RAG)
over my org-mode documents.

#+begin_src python :tangle rag.py
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.core import Document, StorageContext, load_index_from_storage
from llama_index.core.readers.base import BaseReader
from llama_index.core.embeddings import resolve_embed_model
from llama_index.llms.ollama import Ollama
import os, getopt, sys

Settings.embed_model = resolve_embed_model("local:BAAI/bge-small-en-v1.5")
Settings.llm = Ollama(model="mistral", request_timeout=30.0)

def log(message):
  "Prints a message iff verbose is True"
  if verbose:
    print(message)

class DocumentIndex:
  def __init__(self, directory, progress=True, verbose=False):
    self.path = os.path.join(directory, ".llamaindex")
    self.documents = SimpleDirectoryReader(input_dir=directory,
                                           recursive=True,
                                           required_exts=[".org"]).load_data()
    if os.path.exists(self.path):
      context= StorageContext.from_defaults(persist_dir=self.path)
      log("Loading index from disk")
      self.index = load_index_from_storage(context)
    else:
      log("Creating new index")
      self.index = VectorStoreIndex.from_documents(documents, show_progress=progress)
      self.save()

  def refresh(self):
    "Refreshes the index from the updated documents and saves to disk."
    log("Refreshing index with changed documents")
    self.index.refresh(self.documents)
    self.save()

  def save(self):
    "Saves the index to disk under the given directory."
    log("Saving index to disk")
    self.index.storage_context.persist(persist_dir=self.path)

  def print_files(self):
    "Prints the list of all files in the index."
    files = [info.metadata["file_path"] for info in self.index.ref_doc_info.values()]
    print("\n".join(files))

  def query(self, q):
    "Returns the response to the given query."
    return self.index.as_query_engine().query(q)

  def chat(self, mode="context", stream=True):
    engine = self.index.as_chat_engine(chat_mode=mode, streaming=stream)
    engine.streaming_chat_repl()
    return engine

if __name__ == "__main__":
  # default values
  verbose = False
  interactive = False
  refresh = False
  listing = False
  query = ''
  directory = "/Users/christian/Documents/personal/notes/content/"

  arguments = sys.argv[1:]
  short_opts = 'virlq:d:'
  long_opts = ['verbose', 'interactive', 'refresh', 'list', 'query=', 'directory=']

  try:
    opts, _args = getopt.getopt(arguments, short_opts, long_opts)
    for opt, arg in opts:
      if opt in ('-v', '--verbose'):
        verbose = True
      elif opt in ('-i', '--interactive'):
        interactive = True
      elif opt in ('-r', '--refresh'):
        refresh = True
      elif opt in ('-l', '--list'):
        listing = True
      elif opt in ('-q', '--query'):
        query = arg
      elif opt in ('-d', '--directory'):
        directory = arg

    index = DocumentIndex(directory)
    if listing:
      index.print_files()
    elif interactive:
      index.chat()
    elif query:
      print(index.query(query))

    if refresh:
      index.refresh()

    log("Goodbye.")
  except getopt.GetoptError as err:
    print(str(err))
    sys.exit(2)
#+end_src

The following shell script can be called easily (for example from Emacs) and
uses the python program above to perform the RAG query.

#+begin_src shell :tangle ~/bin/org-rag
#!/usr/bin/env bash
conda run -n llama-org-rag python3 "${HOME}/src/projects/llama-org-rag/rag.py" "$@"
#+end_src

*** Impressions
**** Techniques
***** RAG
- simplistic retrieval misses lots of relevant content
- [[https://docs.llamaindex.ai/en/stable/module_guides/observability/observability.html][observability]] is needed to understand cause
  - is it an embedding problem?
    - dissimilarity?
    - cosine distance?
  - is it a retrieval parameter?
    - top_k?
  - are the results discarded post-retrieval?
- good retrieval requires good technique
- can an evaluation step help?
- retrieval as tool for an agent?
**** Tools
***** Vector Stores
****** Chroma DB
- fine for in-memory use only, persistence requires something different
- LlamaIndex's Index abstraction can load/save to/from disk
****** Lance DB
- initialization requires schema or data from which to infer it
***** Frameworks (LangChain v. LlamaIndex)
****** API / Design
- LangChain's API is simpler, but more limiting thab LlamaIndex's
****** Libraries
- [[https://unstructured-io.github.io/unstructured/][unstructured.io]]'s so-called [[https://github.com/Unstructured-IO/unstructured/blob/1947375b2eee8477f7ac95f55783b8262cb90ca9/unstructured/partition/org.py#L4][org-mode support]] is disappointing
  - uses [[https://github.com/JessicaTegner/pypandoc#usage][pypandoc]] under the hood
  - parses as HTML
  - identifies headings and lists, but none of org's richness
    - code blocks, properties, etc
****** Documentation
- LangChain's API docs are [[https://api.python.langchain.com/en/stable/langchain_api_reference.html][well-organized]], readable and link to [[https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent.html#Agent.aplan][source]]
- LLamaIndex's core API docs [[https://docs.llamaindex.ai/en/stable/api_reference/indices/vector_store.html][easy enough]] to read
  - don't link to source
  - LlamaHub community docs are [[https://llamahub.ai/l/readers/llama-index-readers-file?from=readers][not]]
- LlamaIndex's conceptual documentation is thorough, and decently organized
  - structure is not perfectly consistent from section to section
****** Community
- LangChain has lots of [[https://api.python.langchain.com/en/stable/community_api_reference.html#][community packages]]
- LlamaIndex has [[https://llamahub.ai/][LlamaHub]] community package implementations
****** Tools
******* Utilities
- create-llama :: [[https://www.npmjs.com/package/create-llama][node-based]] bootstrapper for LlamaIndex ([[https://blog.llamaindex.ai/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191][blog]], [[https://youtu.be/GOv4arrbVi8?si=9-TEs-_SbKUnhgWx][video]])
******* Observability
- LangSmith :: freemium hosted observability tooling ([[https://docs.smith.langchain.com/][docs]])
  - limit 1 project for free "Developer" plan
- DeepEval :: open-source observability for LLM apps ([[https://github.com/confident-ai/deepeval][Github]], [[https://docs.confident-ai.com/][docs]])
- openllmetry :: freemium? open-source observability ([[https://github.com/traceloop/openllmetry][Github]], [[https://www.traceloop.com/docs/openllmetry/introduction][docs]])
- Arize Phoneix :: ooh pretty! ([[https://github.com/Arize-ai/phoenix][Github]], [[https://docs.arize.com/phoenix][docs]])

*** Future Work
- persist my index to a proper vector database
- periodically update my index `org-rag --refresh`
- convert this to a full-fledged agent with access to tools
  - use LLMCompiler to leverage LLMs planning abilities
  - tools should include Google, Wikipedia, and Wolfram Alpha
  - a basic tool to get the current date and possibly holiday calendars
  - send email or text messages
- wire this up to an Emacs command
- look at [[https://blog.streamlit.io/build-a-chatbot-with-custom-data-sources-powered-by-llamaindex/][different UIs]]
