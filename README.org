#+TITLE: org-mode RAG Project with LlamaIndex
* Overview
This is my second attempt to index my org-mode files to support semantic search
and RAG.

** Vector Database
My previous experiments with Chroma were unsuccessful (embeddings weren't
associated with the org-file content). This version leverages LlamaIndex's
ability to persist an index as files on disk directly without need for a vector
store. Future versions may leverage a proper vector database such as Lance DB,
Qdrant, or Weaviate (embedded).

** LlamaIndex
*** Setup
In a fresh conda environment, install the basic dependencies for this project
with:
#+begin_src shell
pip3 install -r requirements.txt
#+end_src

*** Concepts
**** RAG Phases
***** Ingestion
- loading information from data sources (Documents comprised of Nodes)
- Connectors (aka Readers) know how to ingest particular formats and sources of data
****** Document
- an abstraction that acts like generic container for data
- also tracks metadata and relationships among data (e.g. parent/child or related Docs/Nodes)
****** Node
- a chunk of a source Document
- inherit Document metadata

***** Indexing
- Indexes are data structures that support efficient retrieval and query,
  particularly /semantic/ query
- uses vector Embeddings, which map words to numerical vectors such that related
  concepts are nearby in the vector space
***** Storage
- persist embeddings, content, and metadata durably, often in a vector database
***** Query
- Retrievers fetch context from an index to feed context of an LLM query
- Routers select the optimal retriever from one or more possible choices
- Node Post-processors transform retrieved nodes (e.g. filtering, re-ranking)
- Response synthesizer generates a response from an LLM using a query and
  retrieved data (orchestration?)
**** Composition
***** Query Engine
- end-to-end pipeline for producing a response to a query using an LLM and
  retrieved content
***** Chat Engine
- end-to-end pipeline for having chat (multiple back and forth Q&A)
***** Agents
- automated *decision maker* powered by an LLM that interacts with the world using
  Tools

*** Proof of Concept
The following code uses LlamaIndex to perform Retrieval Augmented Generation (RAG)
over my org-mode documents.

#+begin_src python :tangle rag.py
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.core import StorageContext, load_index_from_storage
from llama_index.core.embeddings import resolve_embed_model
from llama_index.llms.ollama import Ollama
import os

Settings.embed_model = resolve_embed_model("local:BAAI/bge-small-en-v1.5")
Settings.llm = Ollama(model="mistral", request_timeout=30.0)

def acknowledged(question):
  "Returns true if the answer to the question is 'y'."
  answer = input(question +  "(y|n)? " )
  return answer.lower().strip()[0] == "y"

def log(message):
  "Prints a message iff verbose is True"
  if verbose:
    print(message)

class DocumentIndex:
  def __init__(self, directory, progress=True, verbose=False):
    self.path = os.path.join(directory, ".llamaindex")
    self.documents = SimpleDirectoryReader(input_dir=directory,
                                           recursive=True,
                                           required_exts=[".org"]).load_data()
    if os.path.exists(self.path):
      context= StorageContext.from_defaults(persist_dir=self.path)
      log("Loading index from disk")
      self.index = load_index_from_storage(context)
    else:
      log("Creating new index")
      self.index = VectorStoreIndex.from_documents(documents, show_progress=progress)
      self.save()

  def refresh(self):
    log("Refreshing index with changed documents")
    self.index.refresh(self.documents)
    self.save()

  def save(self):
    log("Saving index to disk")
    self.index.storage_context.persist(persist_dir=self.path)

  def chat(self, mode="context", stream=True):
    engine = self.index.as_chat_engine(chat_mode=mode, streaming=stream)
    engine.streaming_chat_repl()
    return engine

if __name__ == "__main__":
  verbose = False
  index = DocumentIndex("/Users/christian/Documents/personal/notes/content/")
  engine = index.chat()

  if acknowledged("Refresh index"):
    index.refresh()

  log("Goodbye.")
#+end_src
